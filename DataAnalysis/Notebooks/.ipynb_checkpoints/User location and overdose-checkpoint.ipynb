{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:51:12.388252Z",
     "start_time": "2019-01-17T16:51:11.069726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:twitterproject) -> /Users/adam/Dropbox/PainNarrativesLab/TwitterProject\n",
      "/Users/adam/Dropbox/PainNarrativesLab/TwitterProject\n",
      "['/Users/adam/.pyenv/versions/3.6.4/lib/python3.6/site-packages/ipykernel_launcher.py', '--config', 'laptop-mining']\n",
      "Reading configuration from /Users/adam/Dropbox/PainNarrativesLab/TwitterProject/configurations/laptop-mining.config.ini\n"
     ]
    }
   ],
   "source": [
    "%cd twitterproject\n",
    "\n",
    "# inject config value (on command line would've been --config=data-analysis)\n",
    "import sys\n",
    "# args = ['--config', 'testing']\n",
    "# args = ['--config', 'data-analysis']\n",
    "args = ['--config', 'laptop-mining']\n",
    "old_sys_argv = sys.argv\n",
    "sys.argv = [old_sys_argv[0]] + args\n",
    "import environment\n",
    "\n",
    "from TwitterDatabase.Repositories import DataRepositories as DR\n",
    "from TwitterDatabase.Repositories import NewOrmRepositories as Repos\n",
    "\n",
    "from TwitterDatabase.DatabaseAccessObjects import DataConnections as DC\n",
    "from TwitterDatabase.Models.WordORM import Word\n",
    "from TwitterDatabase.Models.TweetORM import Users as User\n",
    "from TwitterDatabase.Models.TweetORM import Tweet\n",
    "from DataAnalysis.SearchTools.WordMaps import get_adjacent_word_counts, get_adjacent_words, get_user_ids_for_word\n",
    "\n",
    "EXP_TERMS_FILEPATH = '%s/experimental-terms.xlsx' % environment.EXPERIMENTS_FOLDER\n",
    "IDS_FILEPATH = \"%s/temp_output/user-ids.xlsx\" % environment.LOG_FOLDER_PATH\n",
    "\n",
    "import pandas as pd\n",
    "#Plotting \n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:51:12.447547Z",
     "start_time": "2019-01-17T16:51:12.390403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql+mysqlconnector://hobbes:taco-sauce@localhost/twitter_miner_laptop?charset=utf8mb4\n"
     ]
    }
   ],
   "source": [
    "conn = DC.MySqlConnection(environment.CREDENTIAL_FILE)\n",
    "dao = DC.DAO(conn.engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we find user location with sufficient accuracy to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T02:00:01.729570Z",
     "start_time": "2019-01-15T02:00:01.710653Z"
    }
   },
   "outputs": [],
   "source": [
    "state_abbreviations_file = \"%s/us-states.csv\" % environment.DATA_FOLDER\n",
    "state_abbrevs= pd.read_csv(state_abbreviations_file)\n",
    "state_abbrevs\n",
    "# state_abbrevs = state_abbrevs.Abbreviation.tolist()\n",
    "# state_abbrevs[state_abbrevs.State == 'California']\n",
    "# state_abbrevs[state_abbrevs.Abbreviation == 'AK']\n",
    "# assert( 'AK' in state_abbrevs.Abbreviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T03:33:22.992226Z",
     "start_time": "2019-01-15T03:33:22.947081Z"
    }
   },
   "outputs": [],
   "source": [
    "state_abbreviations_file = \"%s/us-cities.csv\" % environment.DATA_FOLDER\n",
    "state_abbrevs= pd.read_csv(state_abbreviations_file)\n",
    "state_abbrevs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T02:36:55.718305Z",
     "start_time": "2019-01-15T02:36:55.712684Z"
    }
   },
   "source": [
    "# City parsing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:58:30.870959Z",
     "start_time": "2019-01-17T16:58:30.866118Z"
    }
   },
   "outputs": [],
   "source": [
    "from DataAnalysis.SearchTools.LocationFindingTools import StateCityFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:51:25.158096Z",
     "start_time": "2019-01-15T18:51:25.148762Z"
    }
   },
   "outputs": [],
   "source": [
    "testcases_commas = [\n",
    "    # Comma separated\n",
    "    ('Wichita, Kansas', { 'state': 'KS', 'city': 'Wichita' }),\n",
    "    ('Washington, DC', {'state': 'DC', 'city': 'Washington'}),\n",
    "    ('Washington, District of Columbia', {'state': 'DC', 'city': 'Washington'}),\n",
    "    ('New York, NY', {  'state': 'NY', 'city': 'New York' }),\n",
    "    ('Georgia, USA', {'state': 'GA', 'city': None    }),\n",
    "    ('Phoenix, AZ ðŸ‡ºðŸ‡¸', {'state': 'AZ','city': 'Phoenix'}),\n",
    "    ('Phoenix, Arizona ðŸ‡ºðŸ‡¸', {'state': 'AZ','city': 'Phoenix'}),\n",
    "    ('Olney, Maryland, USA', {'state': 'MD','city': 'Olney'}),\n",
    "    ('Joshtown, West Virginia, USA', {'state': 'WV','city': 'Joshtown'}),\n",
    "]\n",
    "\n",
    "# Space separated\n",
    "testcases_spaces = [\n",
    "    ('Wichita Kansas',  {'state': 'KS', 'city': 'Wichita'}),\n",
    "]\n",
    "\n",
    "# State only\n",
    "testcases_state_only = [\n",
    "    ('Kansas', { 'state': 'KS', 'city': None }),\n",
    "    ('West Virginia', { 'state': 'WV', 'city': None }),\n",
    "    ('KS', { 'state': 'KS', 'city': None }),\n",
    "]\n",
    "\n",
    "# City only\n",
    "testcases_city_only = [\n",
    "    ('Seattle', { 'state': 'WA', 'city': 'Seattle' }),\n",
    "    ('New York City', {'state': 'NY', 'city': 'New York'}),\n",
    "    ('NYC',  {'state': 'NY', 'city': 'New York'}),\n",
    "]\n",
    "\n",
    "    # General locale\n",
    "testcases_locales = [    \n",
    "    ('Northern Virginia', {'state': 'VA', 'city': None}),\n",
    "    ('Central NJ, USA', {'state': 'NJ', 'city': None}),\n",
    "    ('No-Cen. Phoenix, AZ', {'state': 'AZ', 'city': 'Phoenix'})\n",
    "]\n",
    "    # Multiple \n",
    "    #             ('DC | Frederick, MD | Chicago'),\n",
    "    #             ('Boston/Providence')]\n",
    "\n",
    "# disambiguate Los angeles and lousiana (LA)\n",
    "\n",
    "alltests = testcases_commas + testcases_spaces + testcases_state_only + testcases_city_only + testcases_locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:51:27.241885Z",
     "start_time": "2019-01-15T18:51:27.170464Z"
    }
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "cf = CityFinder()\n",
    "\n",
    "assert(cf._lookup_statename('Arizona') == 'AZ')\n",
    "assert(cf._lookup_statename('arizona') == 'AZ')\n",
    "assert(cf._lookup_statename('ARIZONA') == 'AZ')\n",
    "assert(cf._lookup_abbrev('AZ') == 'AZ')\n",
    "assert(cf._lookup_abbrev('az') == 'AZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:51:27.712282Z",
     "start_time": "2019-01-15T18:51:27.642072Z"
    }
   },
   "outputs": [],
   "source": [
    "cf = CityFinder()\n",
    "\n",
    "for t in testcases_commas:\n",
    "    try:\n",
    "        assert(cf._detect_comma_separated(t[0]) == t[1])\n",
    "    except AssertionError:\n",
    "        print(t[0], cf._detect_comma_separated(t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:51:28.060029Z",
     "start_time": "2019-01-15T18:51:28.003305Z"
    }
   },
   "outputs": [],
   "source": [
    "cf = CityFinder()\n",
    "\n",
    "for t in testcases_spaces:\n",
    "    try:\n",
    "        assert(cf._detect_space_separated(t[0]) == t[1])\n",
    "    except AssertionError:\n",
    "        print(t[0], cf._detect_space_separated(t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:51:28.441224Z",
     "start_time": "2019-01-15T18:51:28.330728Z"
    }
   },
   "outputs": [],
   "source": [
    "cf = CityFinder()\n",
    "\n",
    "for t in alltests:\n",
    "    try:\n",
    "        assert(cf.parse_location_field(t[0]) == t[1])\n",
    "    except AssertionError:\n",
    "        print(t[0], cf.parse_location_field(t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:21:36.840615Z",
     "start_time": "2019-01-17T16:21:36.837634Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_USERS = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:21:37.276993Z",
     "start_time": "2019-01-17T16:21:37.274776Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#users = dao.session.query(User).limit(NUM_USERS).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:34:56.683564Z",
     "start_time": "2019-01-17T16:22:23.990002Z"
    }
   },
   "outputs": [],
   "source": [
    "cf = StateCityFinder()\n",
    "found = []\n",
    "for u in dao.session.query(User).limit(NUM_USERS).all():\n",
    "    try:\n",
    "        r = cf.parse_location_field(u.location)\n",
    "        if r is not None:\n",
    "            r['userId'] = u.userID\n",
    "            found.append(r)\n",
    "    except:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T19:06:05.256671Z",
     "start_time": "2019-01-15T19:06:05.252246Z"
    }
   },
   "outputs": [],
   "source": [
    "len(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:34:56.792962Z",
     "start_time": "2019-01-17T16:34:56.686349Z"
    }
   },
   "outputs": [],
   "source": [
    "found = pd.DataFrame(found)\n",
    "fg = found.groupby('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:34:57.865053Z",
     "start_time": "2019-01-17T16:34:56.794556Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,5))\n",
    "fg.size().plot(kind='bar', ax=axes)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:51:19.084472Z",
     "start_time": "2019-01-17T16:51:19.080842Z"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T20:34:21.071529Z",
     "start_time": "2019-01-15T20:34:21.068882Z"
    }
   },
   "outputs": [],
   "source": [
    "tid = 1013218633244397568\n",
    "tid = 1013218637782626305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T20:34:21.335375Z",
     "start_time": "2019-01-15T20:34:21.328051Z"
    }
   },
   "outputs": [],
   "source": [
    "t = Repos.get_tweet_by_id(tid, dao.session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T20:34:49.659257Z",
     "start_time": "2019-01-15T20:34:49.651150Z"
    }
   },
   "outputs": [],
   "source": [
    "json.loads(t.other_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:51:21.399628Z",
     "start_time": "2019-01-17T16:51:21.391657Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_possible_location_data_from_tweet(tweet):\n",
    "    \"\"\"If any information exists, we return the whole dictionary\n",
    "    including None and False values. \n",
    "    However, if none of the information is present, we do not return anything (to keep from clutting our output)\n",
    "    \"\"\"\n",
    "    geo_fields = [ 'geo', 'coordinates', 'place']\n",
    "    user_loc_fields = ['location', 'time_zone', 'geo_enabled', 'utc_offset']\n",
    "\n",
    "    def get_profile_location_from_tweet(tweet):\n",
    "        return {'profile_location' : tweet.profile_location}\n",
    "\n",
    "    def get_geo_from_tweet(tweet):\n",
    "        d = json.loads(t.other_data)\n",
    "        return { f : d[f] for f in geo_fields }\n",
    "\n",
    "    def get_user_location_fields_from_tweet(tweet):\n",
    "        d = json.loads(t.other_data)\n",
    "        u = json.loads(d['user'])\n",
    "        return { f : u[f] for f in user_loc_fields }\n",
    "    \n",
    "    a = get_profile_location_from_tweet(tweet)\n",
    "    b = get_geo_from_tweet(tweet)\n",
    "    c = get_user_location_fields_from_tweet(tweet)\n",
    "    r = { **a, **b, **c }\n",
    "    # Filter out any empty results\n",
    "    if len([v for v in r.values() if v and v is not None]) > 0:\n",
    "        # add the tweet and user ids\n",
    "        r['tweetId'] = tweet.tweetID\n",
    "        r['userId'] = tweet.userID\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T21:04:47.241778Z",
     "start_time": "2019-01-16T21:04:47.236198Z"
    }
   },
   "outputs": [],
   "source": [
    "get_all_possible_location_data_from_tweet(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:51:25.178777Z",
     "start_time": "2019-01-17T16:51:25.176255Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_TWEETS = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:54:10.814242Z",
     "start_time": "2019-01-17T16:51:25.556483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296711 results; 29.7% of searched\n",
      "608 with geo field\n",
      "608 with coordinates field\n",
      "266826 with location (from user object) field\n",
      "7467 with full place field\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.sql import func\n",
    "results = []\n",
    "# for t in dao.session.query(Tweet).limit(NUM_TWEETS).all():\n",
    "# random selection: slow!\n",
    "for t in dao.session.query(Tweet).order_by(func.random()).limit(NUM_TWEETS).all():\n",
    "    r = get_all_possible_location_data_from_tweet(t)\n",
    "    if r:\n",
    "        results.append(r)\n",
    "print(\"{} results; {}% of searched\".format(len(results), round((len(results) / NUM_TWEETS)*100, 1)))\n",
    "print(\"%s with geo field\" % len([ r for r in results if r['geo'] is not None ]))\n",
    "print(\"%s with coordinates field\" % len([ r for r in results if r['coordinates'] is not None ]))\n",
    "print(\"%s with location (from user object) field\" % len([ r for r in results if r['location'] is not None and len(r['location']) > 0]))\n",
    "print(\"%s with full place field\" % len([ r for r in results if r['place'] is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T16:54:32.379739Z",
     "start_time": "2019-01-17T16:54:32.371575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'profile_location': None,\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'location': 'Kansas, USA',\n",
       " 'time_zone': None,\n",
       " 'geo_enabled': True,\n",
       " 'utc_offset': None,\n",
       " 'tweetId': 1032071119443034118,\n",
       " 'userId': 2413324957}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-17T16:59:23.729Z"
    }
   },
   "outputs": [],
   "source": [
    "cf = StateCityFinder()\n",
    "\n",
    "found_tweets = []\n",
    "for r in results:\n",
    "#     try:\n",
    "        loc = cf.parse_location_field(r['location'])\n",
    "        if loc is not None:\n",
    "            found_tweets.append(loc)\n",
    "#     except:\n",
    "#         pass\n",
    "len(found_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-17T16:59:26.391Z"
    }
   },
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_tweets = pd.DataFrame(found_tweets)\n",
    "ft = found_tweets.groupby('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,5))\n",
    "ft.size().plot(kind='bar', ax=axes)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " {'profile_location': None,\n",
    "  'geo': None,\n",
    "  'coordinates': None,\n",
    "  'place': '{\"id\": \"010ca0c52ba7443f\", \"url\": \"https://api.twitter.com/1.1/geo/id/010ca0c52ba7443f.json\", \"place_type\": \"city\", \"name\": \"Lenexa\", \"full_name\": \"Lenexa, KS\", \"country_code\": \"US\", \"country\": \"United States\", \"contained_within\": [], \"bounding_box\": {\"type\": \"Polygon\", \"coordinates\": [[[-94.8442025, 38.915313], [-94.704952, 38.915313], [-94.704952, 38.993369], [-94.8442025, 38.993369]]]}, \"attributes\": {}}',\n",
    "  'location': '',\n",
    "  'time_zone': None,\n",
    "  'geo_enabled': True,\n",
    "  'utc_offset': None,\n",
    "  'tweetId': 1013222585226670080,\n",
    "  'userId': 439385224},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
